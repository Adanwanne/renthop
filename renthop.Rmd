---
title: "rent hop"
author: "Adaeze Ajoku"
date: "4/18/2017"
output: html_document
---

```{r loading packages}
library(ggplot2)
library(stringr)
library(tidytext)
library(dplyr)
library(NLP)
library(randomForest)
#library("gplots")
```

```{r reading files}
renthop.tr <- read.csv('/Users/adaezeajoku/Desktop/STATS415/Project/renthop_tr.csv')

renthop.te <- read.csv('/Users/adaezeajoku/Desktop/STATS415/Project/renthop_te.csv')
```

# Step 0: Preparing the Data
```{r data cleaning}
# (1) Remove the brackets
tofix = c(2:5, 9:10, 13:15)
entry_fix <- function(x){
  str_sub(x, 2, -2)
}
train <- data.frame(renthop.tr[,-tofix], apply(renthop.tr[,tofix], 2, entry_fix))
train <- train[,c(1, 7:10, 2:4, 11:12, 5:6, 13:15)]

# (2) Make everything lowercase
text2lower <-  c("display_address", "description", "street_address", "features")   
train[,text2lower] <- sapply(train[,text2lower], tolower) 

# (3) Remove un-needed factors
train$description <- as.character(train$description)
train$features <- as.character(train$features)

# (4) Create a numeric variable for 'photos'
train$photos <- ifelse(str_count(train$photos, ',') == 0, 0, str_count(train$photos, ',') + 1)

# (5) Create a binary variable for 'photos'
train$binary_photos <- factor(ifelse(train$photos == 0, 0, 1))

# (6) Change'building_id' to numeric
train$building_id <- as.numeric(train$building_id)
train$building_id <- ifelse(train$building_id == 1, 0, train$building_id)

# (7) Change 'manager_id' to numeric
train$manager_id <- as.numeric(train$manager_id)

# (8) Create 'hour', 'month', 'year', 'date', and 'day_week' variables from 'created'
train$hour <- as.numeric(str_sub(train$created, -8, -7))
month <- str_sub(train$created, 6, 7)
table(str_sub(train$created, 1,4)) #all the same year, so no invesigation needed
date <- str_sub(train$created, 1, 10)
date <- as.Date(date)
date <- sort(date)
day_week <- rep(0, length(date))

sun <- c(date[1]-5, date[1]-5 + 7, date[1]-5 + 14, date[1]-5 + 21, date[1]-5 + 28, date[1]-5 + 35, date[1]-5 + 42, date[1]-5 + 49, date[1]-5 + 56, date[1]-5 + 63, date[1]-5 + 70, date[1]-5 + 77, date[1]-5 + 84, date[1]-5 + 91)
mon <- c(date[1]-4, date[1]-4 + 7, date[1]-4 + 14, date[1]-4 + 21, date[1]-4 + 28, date[1]-4 + 35, date[1]-4 + 42, date[1]-4 + 49, date[1]-4 + 56, date[1]-4 + 63, date[1]-4 + 70, date[1]-4 + 77, date[1]-4 + 84, date[1]-4 + 91)
tues <- c(date[1]-3, date[1]-3 + 7, date[1]-3 + 14, date[1]-3 + 21, date[1]-3 + 28, date[1]-3 + 35, date[1]-3 + 42, date[1]-3 + 49, date[1]-3 + 56, date[1]-3 + 63, date[1]-3 + 70, date[1]-3 + 77, date[1]-3 + 84, date[1]-3 + 91)
wed <- c(date[1]-2, date[1]-2 + 7, date[1]-2 + 14, date[1]-2 + 21, date[1]-2 + 28, date[1]-2 + 35, date[1]-2 + 42, date[1]-2 + 49, date[1]-2 + 56, date[1]-2 + 63, date[1]-2 + 70, date[1]-2 + 77, date[1]-2 + 84, date[1]-2 + 91)
thurs <- c(date[1]-1, date[1]-1 + 7, date[1]-1 + 14, date[1]-1 + 21, date[1]-1 + 28, date[1]-1 + 35, date[1]-1 + 42, date[1]-1 + 49, date[1]-1 + 56, date[1]-1 + 63, date[1]-1 + 70, date[1]-1 + 77, date[1]-1 + 84, date[1]-1 + 91)
fri <- c(date[1], date[1] + 7, date[1] + 14, date[1] + 21, date[1] + 28, date[1] + 35, date[1] + 42, date[1] + 49, date[1] + 56, date[1] + 63, date[1] + 70, date[1] + 77, date[1] + 84, date[1] + 91)
sat <- c(date[1]+1, date[1]+1 + 7, date[1]+1 + 14, date[1]+1 + 21, date[1]+1 + 28, date[1]+1 + 35, date[1]+1 + 42, date[1]+1 + 49, date[1]+1 + 56, date[1]+1 + 63, date[1]+1 + 70, date[1]+1 + 77, date[1]+1 + 84, date[1]+1 + 91)

day_week <- ifelse(date %in% mon, 'Monday', ifelse(date %in% tues, 'Tuesday', ifelse(date %in% wed, 'Wednesday', ifelse(date %in% thurs, 'Thursday', ifelse(date %in% fri, 'Friday', ifelse(date %in% sat, 'Saturday', ifelse(date %in% sun, 'Sunday', date)))))))

# (9) Prepare 'description' & 'street_address' for tf-idf analysis
train$add_descr <- paste(train$street_address, train$description, sep=" ")

## (a) Remove text between /> and <p><a website_redacted
pattern1 <- '/>[^<>]*<p>'
descr_fix <- str_replace_all(train$add_descr, pattern1, '/> <p>')

## (b) Remove text between '<a' and '</a>'
pattern2 <- '<a.*</a>'
descr_fix <- str_replace_all(descr_fix, pattern2, " ")

## (c) Remove text between '<' and '>'
pattern3 <- '<.*>'
descr_fix <- str_replace_all(descr_fix, pattern3, " ")

## (d) Remove '<a website_redacted'
pattern4 <- '<a.*ted'
descr_fix <- str_replace_all(descr_fix, pattern4, " ")

## (e) Remove phone numbers
pattern5 <- '[0-9]{3}-[0-9]{3}-[0-9]{4}'
descr_fix <- str_replace_all(descr_fix, pattern5, " ")

## (f) Remove email and website
pattern6 <- '[a-z]+[\\.@].*?[a-z]{3}( |;|\\/[a-z]+|")'
descr_fix <- str_replace_all(descr_fix, pattern6, " ")

pattern6a <- '[a-z]+[\\.@].*?[a-z]{3}'
descr_fix <- str_replace_all(descr_fix, pattern6a, " ")

## (g) Remove 'website_redacted'
pattern7 <- 'website_redacted'
descr_fix <- str_replace_all(descr_fix, pattern7, "")

## (h) Remove prices
pattern8 <- '\\$[0-9]+( |.[0-9]+)'
descr_fix <- str_replace_all(descr_fix, pattern8, " ")

## (i) Remove '&amp'
pattern9 <- '&amp'
descr_fix <- str_replace_all(descr_fix, pattern9, " ")

## (j) Strip all non-alphanumeric characters
pattern10 <- "[^a-z0-9'/-]"
descr_fix <- str_replace_all(descr_fix, pattern10, " ")

descr_fix <- str_replace_all(descr_fix, ' amp ', "")

## (k) Remove character that repeats 4+ times consecutively 
pattern11 <- '[dgtwz]{4,}'
descr_fix<- str_replace_all(descr_fix, pattern11, " ")
descr_fix[20052] <- str_sub(descr_fix[20052], 1, 21)
descr_fix[c(23913, 39444, 41526)] <- str_sub(descr_fix[c(23913, 39444, 41526)], 1, -15)

## (l) Remove remaining punctuation marks of /,',- not between letters or numbers
pattern12a <- '(?<![a-z0-9])[[:punct:]]' 
pattern12b <- '[[:punct:]](?![a-z0-9])'
descr_fix  <- gsub(pattern12a, " ", descr_fix, perl=TRUE)
descr_fix  <- gsub(pattern12b, " ", descr_fix, perl=TRUE)

## (m) Reduce 2+ spaces to just one
pattern13 <- " {2,}"
descr_fix <- str_replace_all(descr_fix, pattern13, " ")

for(i in 1:length(descr_fix)){
  if(str_sub(descr_fix[i], start=-1) == " ")
    {descr_fix[i] <- str_sub(descr_fix[i], 1, -2)}
}

train$add_descr <- descr_fix

#Decided against removing numbers because have info like m20 bus, 87th ave, etc. Decided against removing stop words since tf-idf will automatically scale down words that appear in many documents such as 'the'. I will check the performance and reconsider this.

# (10) Prepare 'features' for tf-idf analysis
## (a) Strip all non-alphanumeric characters
feat_fix <- str_replace_all(train$features, pattern10, " ")

## (b) Reduce 2+ spaces to just one
feat_fix <- str_replace_all(feat_fix, pattern13, " ")

for(i in 1:length(feat_fix)){
  if(str_sub(feat_fix[i], start=-1) == " ")
    {feat_fix[i] <- str_sub(feat_fix[i], 1, -2)}
}

train$feat_fix <- feat_fix
```

# Step 1: Exploratory Data Analysis
Do as much data exploration as possible by visualization and summary statistics. 

## Important Notes
```{r notes}
# R View of dataframe is misleading. It shows '[]' because entry is too long to display.

# The following columns will not be used: display_address. 

# The following columns have been added: binary_photos
```

## Addressing Preliminary Questions
```{r interest level}
#' How many observations of each interest level are there?
table(train$interest_level)
# 7.8% high (3839), 69.5% low (34284), 22.8% medium (11229)
```

```{r description: one word tfidf}
trim <- function (x) gsub("^[[:space:]]+|[[:space:]]+$", "", x) 
#remove leading or trailing white space

high_descr_char <- paste(train$add_descr[train$interest_level=='high'], sep=" ")
high_descr_dat <- data.frame(`word` = unlist(strsplit(high_descr_char, " ")), `n` = rep(1, 212505))
high_descr_dat$word <- trim(high_descr_dat$word)
high_descr_dat <- setNames(aggregate(high_descr_dat$n, by=list(high_descr_dat$word), FUN='sum'), c('word', 'n'))
high_descr_dat <- high_descr_dat[-1,]
high_descr_dat$total <- rep(sum(high_descr_dat$n), nrow(high_descr_dat))
high_descr_dat$`interest_level` <- rep('high', nrow(high_descr_dat))

medium_descr_char <- paste(train$add_descr[train$interest_level=='medium'], sep=" ")
medium_descr_dat <- data.frame(`word` = unlist(strsplit(medium_descr_char, " ")), `n` = rep(1, 633902))
medium_descr_dat$word <- trim(medium_descr_dat$word)
medium_descr_dat <- setNames(aggregate(medium_descr_dat$n, by=list(medium_descr_dat$word), FUN='sum'), c('word', 'n'))
medium_descr_dat <- medium_descr_dat[-1,]
medium_descr_dat$total <- rep(sum(medium_descr_dat$n), nrow(medium_descr_dat))
medium_descr_dat$`interest_level` <- rep('medium', nrow(medium_descr_dat))

low_descr_char <- paste(train$add_descr[train$interest_level=='low'], sep=" ")
low_descr_dat <- data.frame(`word` = unlist(strsplit(low_descr_char, " ")), `n` = rep(1, 1823704))
low_descr_dat$word <- trim(low_descr_dat$word)
low_descr_dat <- setNames(aggregate(low_descr_dat$n, by=list(low_descr_dat$word), FUN='sum'), c('word', 'n'))
low_descr_dat <- low_descr_dat[-c(1:2),]
low_descr_dat$total <- rep(sum(low_descr_dat$n), nrow(low_descr_dat))
low_descr_dat$`interest_level` <- rep('low', nrow(low_descr_dat))

descr_dat <- rbind(high_descr_dat, medium_descr_dat)
descr_dat <- rbind(descr_dat, low_descr_dat)

ggplot(descr_dat, aes(n/total, fill = interest_level)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~interest_level, ncol = 2, scales = "free_y")

descr_dat <- descr_dat %>% 
  bind_tf_idf(word, interest_level, n)
#other package to set weight for if_idf?

descr_dat %>%
  filter(`interest_level` == 'high') %>% 
  select(-total) %>%
  arrange(desc(tf_idf)) %>% 
  View()

#hm. not great findings with just one word. try creating bigrams.
```

```{r bigram approach}
ngram_maker <- function(x,c){
  s1 <- ngrams(x, c)
  s2 <- vapply(s1, paste, "", collapse = " ") 
  s2
}

high_descr_word <- unlist(strsplit(high_descr_char, " "))
high_descr_word <- trim(high_descr_word)
high_descr_word <-  high_descr_word[high_descr_word != ""]
high_descr_dat2 <- data.frame(`bigram` = ngram_maker(high_descr_word, 2L), `n` = rep(1, 223954))
high_descr_dat2 <- setNames(aggregate(high_descr_dat2$n, by=list(high_descr_dat2$bigram), FUN='sum'), c('bigram', 'n'))
high_descr_dat2$total <- rep(sum(high_descr_dat2$n), nrow(high_descr_dat2))
high_descr_dat2$`interest_level` <- rep('high', nrow(high_descr_dat2))

medium_descr_word <- unlist(strsplit(medium_descr_char, " "))
medium_descr_word <- trim(medium_descr_word)
medium_descr_word <-  medium_descr_word[medium_descr_word != ""]
medium_descr_dat2 <- data.frame(`bigram` = ngram_maker(medium_descr_word, 2L), `n` = rep(1, 666555))
medium_descr_dat2 <- setNames(aggregate(medium_descr_dat2$n, by=list(medium_descr_dat2$bigram), FUN='sum'), c('bigram', 'n'))
medium_descr_dat2$total <- rep(sum(medium_descr_dat2$n), nrow(medium_descr_dat2))
medium_descr_dat2$`interest_level` <- rep('medium', nrow(medium_descr_dat2))

low_descr_word <- unlist(strsplit(low_descr_char, " "))
low_descr_word <- trim(low_descr_word)
low_descr_word <-  low_descr_word[low_descr_word != ""]
low_descr_dat2 <- data.frame(`bigram` = ngram_maker(low_descr_word, 2L), `n` = rep(1, 1928101))
low_descr_dat2 <- setNames(aggregate(low_descr_dat2$n, by=list(low_descr_dat2$bigram), FUN='sum'), c('bigram', 'n'))
low_descr_dat2$total <- rep(sum(low_descr_dat2$n), nrow(low_descr_dat2))
low_descr_dat2$`interest_level` <- rep('low', nrow(low_descr_dat2))

descr_dat2 <- rbind(high_descr_dat2, medium_descr_dat2)
descr_dat2 <- rbind(descr_dat2, low_descr_dat2)

ggplot(descr_dat2, aes(n/total, fill = interest_level)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~interest_level, ncol = 2, scales = "free_y")

descr_dat2 <- descr_dat2 %>% 
  bind_tf_idf(bigram, interest_level, n)
#other package to set weight for if_idf?

high_descr_tfidf <- descr_dat2 %>%
  filter(`interest_level` == 'high') %>% 
  arrange(desc(tf_idf)) 

medium_descr_tfidf <- descr_dat2 %>%
  filter(`interest_level` == 'medium') %>% 
  arrange(desc(tf_idf)) 

low_descr_tfidf <- descr_dat2 %>%
  filter(`interest_level` == 'low') %>% 
  arrange(desc(tf_idf)) 

high_bigrams <- as.character(high_descr_tfidf$bigram[1:100])
medium_bigrams <- as.character(medium_descr_tfidf$bigram[1:100])
low_bigrams <- as.character(low_descr_tfidf$bigram[1:100])

descr_names <- unique(c(high_bigrams, medium_bigrams, low_bigrams))
#ran into an issue with duplicate bigrams within the description. I used unique to remove duplicates.
train2 <- data.frame(matrix(ncol=length(descr_names), nrow=nrow(train)))
colnames(train2) <- descr_names
train2 <- cbind(train[,c('listing_id', 'interest_level', 'price', 'binary_photos', 'bedrooms', 'bathrooms', 'hour', 'building_id', 'manager_id')], train2)

for(i in 10:length(train2)){
  train2[, i] <- ifelse(grepl(colnames(train2)[i],train$add_descr) == TRUE, 1, 0) 
}

#To entirely prevent the occurrence of duplicates I would have to grab only 10 bigrams from each category. I can check to see if this will have similar performance as grabbing 100.
high_bigrams2 <- as.character(high_descr_tfidf$bigram[1:10])
medium_bigrams2 <- as.character(medium_descr_tfidf$bigram[1:10])
low_bigrams2 <- as.character(low_descr_tfidf$bigram[1:10])

descr_names2 <- unique(c(high_bigrams2, medium_bigrams2, low_bigrams2))

train3 <- data.frame(matrix(ncol=length(descr_names2), nrow=nrow(train)))
colnames(train3) <- descr_names2
train3 <- cbind(train[,c('listing_id', 'interest_level', 'price', 'binary_photos', 'bedrooms', 'bathrooms', 'hour', 'building_id', 'manager_id')], train3)

for(i in 10:length(train3)){
  train3[, i] <- ifelse(grepl(colnames(train3)[i],train$add_descr) == TRUE, 1, 0) 
}
```

```{r chi-square test}
#This tests for independence. However, I want the bigrams to be correlated with the interest_level.

descr_tbl <- table(descr_dat2$bigram, descr_dat2$interest_level)
chisq.test(descr_tbl) 
chisq.test(matrix(descr_tbl))
#using all bigrams I get a p-value of 1 indicating that the bigrams are independent of the interest level. However, because some bigrams have a very low frequency, the chi-squared approximation may be incorrect. To account for this I will remove tf_idf that are below the mean? [Did so and got a chi-squared of NA]
chisq.test(descr_dat2$bigram, descr_dat2$interest_level)
```

```{r created}
# We don't have information on when/if a posting has become inactive. 

#' Is there a relationship between posting hour and interest level?
chisq.test(table(train$hour, train$interest_level))$resid  
#percentages? no, i think the test can handle this...
#The chi-squared test indicates significant correlation

#' Is there a relationship between posting month and interest level? [We have months: Apr-June]
chisq.test(table(month, train$interest_level))
# p-value is 0.3784, so no.

#' Is there a relationship between day of the week and interest level?
chisq.test(table(day_week, train$interest_level))
#p-value is 0.682, so no.

#We don't have enough (repeating) dates, to examine that.
```

```{r price}
#' What is the distribution of price by interest level?
boxplot(price~interest_level, data=train[train$price < 80000,]) #eliminated some severe outliers for a better visualization!
tapply(train$price, train$interest_level, summary)
#mean prices: high = 2700.293, low=4176.599, medium=3158.767
#So people are more interested the lower the price.

#' How does price and interest_level relate to listing location?
pb <- data.frame(table(train$price, train$building_id))
```

```{r bedrooms}
boxplot(bedrooms~interest_level, data=train)
```

```{r longitude and latitude}
#Fix graph, not showing enough points

#' What is the lat/long for listings by interest_level? Are there clusters of regions by interest_level?
ggplot(train, aes(longitude, latitude, color=interest_level)) + geom_point(alpha=0.5) + geom_jitter(width=15)
#No so using longitude and latitude will not be beneficial, there's no real differentiation of points. And the 0,0 points are missing.
```

```{r photos}
#' Does no photo translate to low interest?
table(train$interest_level[train$photos == 0])
#' Yes 91.4% of the time.

#' How do number of photos relate to interest level?
boxplot(photos~interest_level, data=train)
# For all interest levels, the distribution of photos in the ad seems equal. So perhaps having photo be a binary variable is a better option?
```

```{r manager id}
#' Are there certain undesirable managers?
head(table(train$manager_id)[order(table(train$manager_id), decreasing = T)])
manager_feels <- data.frame(table(train$manager_id, train$interest_level))
names(manager_feels) <- c('manager_id', 'interest_level', 'freq')
manager_feels <- manager_feels[manager_feels$freq != 0, ]
mean(manager_feels$freq)
#Mean frequency for a manager is 8.6. Consider managers who have frequencies of 9 or greater.
manager_feels <- manager_feels[manager_feels$freq >= 9, ]
#how to see frequency for each interest_level by manager_id?
```

```{r bathrooms}
boxplot(bathrooms~interest_level, data=train[train$price < 80000,])
#basically all have 1 bathroom
```

```{r building id}
#Q: Are building ids unique to property managers/co?
bm <- data.frame(table(train$building_id, train$manager_id))
bm %>% 
  filter(Var1 == 1) %>% 
  View()
#No, one building can have numerous units with different managers.

#' Are there more/less desirable buildings?
building_feels <- data.frame(table(train$building_id, train$interest_level))
names(building_feels) <- c('building_id', 'interest_level', 'freq')
building_feels <- building_feels[building_feels$freq != 0, ]
mean(building_feels$freq)
#Mean frequency for a manager is 3.68. Consider managers who have frequencies of 4 or greater.
building_feels <- building_feels[building_feels$freq >= 4, ]
```

```{r street address}
#' Which streets are most common in listings?
streets <- table(train$display_address)
head(streets[order(streets, decreasing = T)])
#' Broadway, East 34th Street, Second Ave

#' What is the relationship btw street address and building id?
View(data.frame(train$street_address, train$building_id))
#for many street addresses, the same address corresponds to the same building id. Some discrepancies occur from missing building_id or street address that include apt. no.
id2street <- paste(train$street_address, train$building_id, sep=" ")
View(table(id2street)) #checking with bigrams

#' What is the frequency of street addresses by interest level?
View(table(train$street_address, train$interest_level))
#505 west 37th street mostly appears in low interest listings (114)
```

```{r features}
#' Do no features translate to low interest?
table(train$interest_level[train$features == ''])
#' Yes 65% of the time.

#' What are typical features offered? By interest_level?
#Hardwood Floors are the most frequent feature of high interest listings occurring 1986 times. Assuming the feature does not occur multiple times in the same listing that comprises, 51.7% of high interest listing.
high_feat_char <- paste(train$feat_fix[train$interest_level=='high'], sep=" ")
high_feat_word <- unlist(strsplit(high_feat_char, " "))
high_feat_word <- trim(high_feat_word)
high_feat_word <-  high_feat_word[high_feat_word != ""]
high_feat_dat <- data.frame(`bigram` = ngram_maker(high_feat_word, 2L), `n` = rep(1, 36504))
high_feat_dat <- setNames(aggregate(high_feat_dat$n, by=list(high_feat_dat$bigram), FUN='sum'), c('bigram', 'n'))
high_feat_dat$total <- rep(sum(high_feat_dat$n), nrow(high_feat_dat))
high_feat_dat$`interest_level` <- rep('high', nrow(high_feat_dat))

medium_feat_char <- paste(train$feat_fix[train$interest_level=='medium'], sep=" ")
medium_feat_word <- unlist(strsplit(medium_feat_char, " "))
medium_feat_word <- trim(medium_feat_word)
medium_feat_word <-  medium_feat_word[medium_feat_word != ""]
medium_feat_dat <- data.frame(`bigram` = ngram_maker(medium_feat_word, 2L), `n` = rep(1, 120435))
medium_feat_dat <- setNames(aggregate(medium_feat_dat$n, by=list(medium_feat_dat$bigram), FUN='sum'), c('bigram', 'n'))
medium_feat_dat$total <- rep(sum(medium_feat_dat$n), nrow(medium_feat_dat))
medium_feat_dat$`interest_level` <- rep('medium', nrow(medium_feat_dat))

low_feat_char <- paste(train$feat_fix[train$interest_level=='low'], sep=" ")
low_feat_word <- unlist(strsplit(low_feat_char, " "))
low_feat_word <- trim(low_feat_word)
low_feat_word <-  low_feat_word[low_feat_word != ""]
low_feat_dat <- data.frame(`bigram` = ngram_maker(low_feat_word, 2L), `n` = rep(1, 318810))
low_feat_dat <- setNames(aggregate(low_feat_dat$n, by=list(low_feat_dat$bigram), FUN='sum'), c('bigram', 'n'))
low_feat_dat$total <- rep(sum(low_feat_dat$n), nrow(low_feat_dat))
low_feat_dat$`interest_level` <- rep('low', nrow(low_feat_dat))

feat_dat <- rbind(high_feat_dat, medium_feat_dat)
feat_dat <- rbind(feat_dat, low_feat_dat)

ggplot(feat_dat, aes(n/total, fill = interest_level)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~interest_level, ncol = 2, scales = "free_y")

feat_dat <- feat_dat %>% 
  bind_tf_idf(bigram, interest_level, n)
#other package to set weight for if_idf?

high_feat_tfidf <- feat_dat %>%
  filter(`interest_level` == 'high') %>% 
  arrange(desc(tf_idf)) 

medium_feat_tfidf <- feat_dat %>%
  filter(`interest_level` == 'medium') %>% 
  arrange(desc(tf_idf)) 

low_feat_tfidf <- feat_dat %>%
  filter(`interest_level` == 'low') %>% 
  arrange(desc(tf_idf)) 

feath_bigrams <- as.character(high_feat_tfidf$bigram[1:25])
featm_bigrams <- as.character(medium_feat_tfidf$bigram[1:25])
featl_bigrams <- as.character(low_feat_tfidf$bigram[1:25])

feat_names <- unique(c(paste('f', feath_bigrams, sep= " "), paste('f', featm_bigrams, sep=" "), paste('f', featl_bigrams, sep=" ")))
#ran into an issue with duplicate bigrams within the features. I used unique to remove duplicates. Also I added 'f' to distinguish feature bigrams from description bigrams.

feat_bigrams <- data.frame(matrix(ncol=length(feat_names), nrow=nrow(train2)))
colnames(feat_bigrams) <- feat_names
train2 <- cbind(train2, feat_bigrams)

for(i in 301:length(train2)){
  train2[, i] <- ifelse(grepl(str_sub(colnames(train2)[i], 3), train$feat_fix) == TRUE, 1, 0) 
}

#To entirely prevent the occurrence of duplicates I would have to grab only 2 bigrams from each category. I can check to see if this will have similar performance as grabbing 25.
feath_bigrams2 <- as.character(high_feat_tfidf$bigram[1:2])
featm_bigrams2 <- as.character(medium_feat_tfidf$bigram[1:2])
featl_bigrams2 <- as.character(low_feat_tfidf$bigram[1:2])

feat_names2 <- unique(c(paste('f', feath_bigrams2, sep= " "), paste('f', featm_bigrams2, sep=" "), paste('f', featl_bigrams2, sep=" ")))
#ran into an issue with duplicate bigrams within the features. I used unique to remove duplicates. Also I added 'f' to distinguish feature bigrams from description bigrams.

feat_bigrams2 <- data.frame(matrix(ncol=length(feat_names2), nrow=nrow(train)))
colnames(feat_bigrams2) <- feat_names2

#Only 10 description bigrams + all 25 feature bigrams:
train3 <- cbind(train3, feat_bigrams)
for(i in 40:length(train3)){
  train3[, i] <- ifelse(grepl(str_sub(colnames(train3)[i], 3), train$feat_fix) == TRUE, 1, 0) 
}

#All 300 description bigrams + only 2 feature bigrams:
train3a <- cbind(train2[,-c(301:372)], feat_bigrams2)
for(i in 301:length(train3a)){
  train3a[, i] <- ifelse(grepl(str_sub(colnames(train3a)[i], 3), train$feat_fix) == TRUE, 1, 0) 
}

#Only 10 description bigrams + 2 feature bigrams:
train3b <- cbind(train3[,-c(40:111)], feat_bigrams2)
for(i in 40:length(train3b)){
  train3b[, i] <- ifelse(grepl(str_sub(colnames(train3b)[i], 3), train$feat_fix) == TRUE, 1, 0) 
}
```

# Step 2: Conclusions from Exploratory Data Analysis

dt <- table(manager_feels)
# 2. Graph
balloonplot(dt, main ="manager interest levels", xlab ="", ylab="", label = FALSE, show.margins = FALSE)

library(corrplot)
corrplot(chisq$residuals, is.cor = FALSE)

# Step 3: Statistical Learning Methods
```{r prepping the test set}
# (1) Remove the brackets
tofix = c(2:5, 9:10, 13:15)

test <- data.frame(renthop.te[,-tofix], apply(renthop.te[,tofix], 2, entry_fix))
test <- test[,c(1, 7:10, 2:4, 11:12, 5:6, 13:15)]

# (2) Make everything lowercase
test[,text2lower] <- sapply(test[,text2lower], tolower) 

# (3) Remove un-needed factors
test$description <- as.character(test$description)
test$features <- as.character(test$features)

# (4) Create a numeric variable for 'photos'
test$photos <- ifelse(str_count(test$photos, ',') == 0, 0, str_count(test$photos, ',') + 1)

# (5) Create a binary variable for 'photos'
test$binary_photos <- factor(ifelse(test$photos == 0, 0, 1))

# (6) Change'building_id' to numeric
test$building_id <- as.numeric(test$building_id)
test$building_id <- ifelse(test$building_id == 1, 0, test$building_id)

# (7) Change 'manager_id' to numeric
test$manager_id <- as.numeric(test$manager_id)

# (8) Create 'hour', 'month', 'year', 'date', and 'day_week' variables from 'created'
test$hour <- as.numeric(str_sub(test$created, -8, -7))
table(str_sub(test$created, 6, 7))
table(str_sub(test$created, 1,4)) #all the same year, so no invesigation needed

# (9) Prepare 'description' & 'street_address' for tf-idf analysis
test$add_descr <- paste(test$street_address, test$description, sep=" ")

## (a) Remove text between /> and <p><a website_redacted
descr_fixt <- str_replace_all(test$add_descr, pattern1, '/> <p>')

## (b) Remove text between '<a' and '</a>'
descr_fixt <- str_replace_all(descr_fixt, pattern2, " ")

## (c) Remove text between '<' and '>'
descr_fixt <- str_replace_all(descr_fixt, pattern3, " ")

## (d) Remove '<a website_redacted'
descr_fixt <- str_replace_all(descr_fixt, pattern4, " ")

## (e) Remove phone numbers
descr_fixt <- str_replace_all(descr_fixt, pattern5, " ")

## (f) Remove email and website
descr_fixt <- str_replace_all(descr_fixt, pattern6, " ")
descr_fixt <- str_replace_all(descr_fixt, pattern6a, " ")

## (g) Remove 'website_redacted'
descr_fixt <- str_replace_all(descr_fixt, pattern7, "")

## (h) Remove prices
descr_fixt <- str_replace_all(descr_fixt, pattern8, " ")

## (i) Remove '&amp'
descr_fixt <- str_replace_all(descr_fixt, pattern9, " ")

## (j) Strip all non-alphanumeric characters
descr_fixt <- str_replace_all(descr_fixt, pattern10, " ")
descr_fixt <- str_replace_all(descr_fixt, ' amp ', "")

## (k) Remove character that repeats 4+ times consecutively CHECK!!
pattern11 <- '[dgtwz]{4,}'
descr_fix<- str_replace_all(descr_fix, pattern11, " ")
descr_fix[20052] <- str_sub(descr_fix[20052], 1, 21)
descr_fix[c(23913, 39444, 41526)] <- str_sub(descr_fix[c(23913, 39444, 41526)], 1, -15)

## (l) Remove remaining punctuation marks of /,',- not between letters or numbers
descr_fixt  <- gsub(pattern12a, " ", descr_fixt, perl=TRUE)
descr_fixt  <- gsub(pattern12b, " ", descr_fixt, perl=TRUE)

## (m) Reduce 2+ spaces to just one
descr_fixt <- str_replace_all(descr_fixt, pattern13, " ")

for(i in 1:length(descr_fixt)){
  if(str_sub(descr_fixt[i], start=-1) == " ")
    {descr_fixt[i] <- str_sub(descr_fixt[i], 1, -2)}
}

test$add_descr <- descr_fixt

#Decided against removing numbers because have info like m20 bus, 87th ave, etc. Decided against removing stop words since tf-idf will automatically scale down words that appear in many documents such as 'the'. I will check the performance and reconsider this.

# (10) Prepare 'features' for tf-idf analysis
## (a) Strip all non-alphanumeric characters
feat_fix <- str_replace_all(train$features, pattern10, " ")

## (b) Reduce 2+ spaces to just one
feat_fix <- str_replace_all(feat_fix, pattern13, " ")

for(i in 1:length(feat_fix)){
  if(str_sub(feat_fix[i], start=-1) == " ")
    {feat_fix[i] <- str_sub(feat_fix[i], 1, -2)}
}

train$feat_fix <- feat_fix
```


```{r random forest}
require(rpart)
require(ISLR)
require(e1071)
require(gbm)

set.seed(357)

errors = rep(NA, 5)
idx = 1
for(i in c(1,25,50,75,100)){
  classRF = randomForest(x=train2[,-c(1:2)], y=train2[,2], ntree = 100, nodesize = i, importance = TRUE)
  errors[idx] =mean(predict(classRF, train2[,-c(1:2)]) != train2[,2])
  idx = idx + 1
}
errors
#so a node size of around 25-50 seems optimal. I will check 19 since sqrt(371) = 19.

classRF = randomForest(x=train2[,-c(1:2)], y=train2[,2], ntree = 100, nodesize = 19, importance = TRUE)
mean(predict(classRF, train2[,-c(1:2)]) != train2[,2])
#for 19, the training error is 0.281 :/

#Checking error with reduced bigram count
classRF = randomForest(x=train3[,-c(1:2)], y=train3[,2], ntree = 100, nodesize = 19, importance = TRUE)
mean(predict(classRF, train3[,-c(1:2)]) != train3[,2])
#for red descr, full features: 0.296

classRF = randomForest(x=train3a[,-c(1:2)], y=train3a[,2], ntree = 100, nodesize = 19, importance = TRUE)
mean(predict(classRF, train3a[,-c(1:2)]) != train3a[,2])
#for full descr, red features: 0.279

classRF = randomForest(x=train3b[,-c(1:2)], y=train3b[,2], ntree = 100, nodesize = 19, importance = TRUE)
mean(predict(classRF, train3b[,-c(1:2)]) != train3b[,2])
#for red descr, red features: 0.287

#So I will use the dataframe with 100 description bigrams and 2 feature bigrams. 
#Now to optimize the nodesize:
errors2 = rep(NA, 6)
idx = 1
for(i in c(25, 30, 35, 40, 45, 50)){
  classRF = randomForest(x=train3a[,-c(1:2)], y=train3a[,2], ntree = 100, nodesize = i, importance = TRUE)
  errors2[idx] =mean(predict(classRF, train3a[,-c(1:2)]) != train3a[,2])
  idx = idx + 1
}
errors2 #optimal nodesize lies in (25, 30)

errors3 = rep(NA, 4)
idx = 1
for(i in c(26, 27, 28, 29)){
  classRF = randomForest(x=train3a[,-c(1:2)], y=train3a[,2], ntree = 100, nodesize = i, importance = TRUE)
  errors3[idx] =mean(predict(classRF, train3a[,-c(1:2)]) != train3a[,2])
  idx = idx + 1
}
errors3

tuneRF #to find optimal parameters

#Get a measure of variable importance. High MeanDecreaseGini values imply that the variable is important, and low ones imply it is not.
classRF$importance

```

```{r other}
# Bagging and random forest are just two specific cases of ensembling models, where you take different models, and combine their outputs to get your final prediction. This generally produces an improvement over the models individually as long as the individual models are doing equally well. 

regSVM = svm(mpg ~ ., data = Auto, kernel = 'radial', cost = 10, gamma = 1)

print(mean((predict(regSVM, Auto[,-1]) - Auto[,1] )^2))
print(mean((predict(regRF, Auto[,-1]) - Auto[,1] )^2))

print(mean(( 0.5*(predict(regRF, Auto[,-1]) + predict(regSVM, Auto[,-1])) - Auto[,1] )^2)) #better MSE by taking the average of the two (but generally the two methods must be decent on their own to see an improvement in the combination)

#############################################################################

# Boosting

# Final boosting is similar to the methods we have seen already, except while in bagging we wanted our learners to be independent, in boosting we want our learners to be strongly dependent on each other. In boosting we learn sequentially, where the next tree is fit on the residuals of the last one (ie its errors). We then add all the learners together. This slowly builds a complex learner, and tends to do a good job of not overfitting. (If we factor in a shrinkage factor [weigh the later models less] then we can prevent overfitting to the hard to fit points)

regBoost = gbm(mpg ~ ., data = Auto, distribution = 'gaussian', n.trees = 10000, interaction.depth = 3)
summary(regBoost)

print(mean(( predict(regBoost, Auto[,-1], n.trees = 10000) - Auto[,1] )^2))

set.seed(1)

folds = 5

foldID = rep(1:folds, length.out = nrow(Auto))
foldID = sample(foldID) # permuting our set

CVmse = matrix(nrow = 3, ncol = folds)

for(i in 1:folds){
  
  train = Auto[which(foldID != i),]

  
  testX = Auto[which(foldID == i),-1]
  testY = Auto[which(foldID == i),1]
  
  tempSVM = svm(mpg ~ ., data = train, kernel = 'radial', cost = 10, gamma = 1)
  CVmse[1,i] = mean((predict(tempSVM,testX) - testY)^2)
  
  tempRF = randomForest(mpg ~ ., data = train, ntree = 500, importance = TRUE, nodesize = 2)
  CVmse[2,i] = mean((predict(tempRF,testX) - testY)^2)
  
  tempBoost = gbm(mpg ~ ., data = train, distribution = 'gaussian', n.trees = 10000, interaction.depth = 3)
  CVmse[3,i] = mean((predict(tempBoost,testX, n.trees = 10000) - testY)^2)
    
}

View(CVmse)

# This suggests that the SVM was overfitting the data a lot, and the RF and boosting trees were not overfitting as much. This is often an advantage of these ensemble algorithms which use many weak learners. They can avoid overfitting more than using a complex model like an SVM. 

```

```{r misc code}
pattern11c <- "(?!('|/|-))[[:punct:]]"
descr_fix  <- gsub(pattern11c, " ", descr_fix, perl=TRUE)

## (l) Remove tab characters
pattern12 <- "\t+"
line <- '\tg\t\t122hhh'
descr_fix <- str_replace_all(descr_fix, pattern12, "")
```
